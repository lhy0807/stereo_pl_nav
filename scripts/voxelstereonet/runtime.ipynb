{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28895b1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:89: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (3.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "import os\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import DataLoader\n",
    "from utils import *\n",
    "from utils.KittiColormap import *\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from datasets.data_io import get_transform, read_all_lines, pfm_imread\n",
    "from PIL import Image\n",
    "from tqdm import tqdm, trange\n",
    "from models.mobilestereonet.models.MSNet2D import MSNet2D\n",
    "from models.mobilestereonet.models.MSNet3D import MSNet3D\n",
    "from models.Voxel2D import Voxel2D\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import coloredlogs, logging\n",
    "from datasets import VoxelDSDataset\n",
    "import torch.nn.functional as F\n",
    "import traceback\n",
    "from torchmetrics.functional import jaccard_index\n",
    "from pytorch3d.loss import chamfer_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47c6918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_img = torch.rand((1,3, 400, 880))\n",
    "right_img = torch.rand((1,3, 400, 880))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930338df",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MSNet2D(192)\n",
    "model = nn.DataParallel(model)\n",
    "ckpt_path = \"../models/MSNet2D_SF_DS.ckpt\"\n",
    "print(\"Loading model {}\".format(ckpt_path))\n",
    "state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42d3974",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.cpu()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd33183",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 2 -r 5 model(left_img, right_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7f1b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.CFNet.models import CFNet\n",
    "model = CFNet(192)\n",
    "model = nn.DataParallel(model)\n",
    "ckpt = torch.load(\"models/CFNet/finetuning_model\", map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt['model'])\n",
    "model.cpu()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5391532",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_img = torch.rand((1,3, 384, 864))\n",
    "right_img = torch.rand((1,3, 384, 864))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e24e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 2 -r 5 model(left_img, right_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03e2900",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ACVNet.models import ACVNet\n",
    "model = ACVNet(192, False, False)\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "ckpt = torch.load(\"models/ACVNet/pretrained_model/checkpoint_000009.ckpt\", map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"model\"])\n",
    "model.cpu()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b416ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 2 -r 5 model(left_img, right_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35304a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.lacGwcNet.networks.stackhourglass import PSMNet\n",
    "# load model\n",
    "affinity_settings = {}\n",
    "affinity_settings['win_w'] = 3\n",
    "affinity_settings['win_h'] = 3\n",
    "affinity_settings['dilation'] = [1, 2, 4, 8]\n",
    "model = PSMNet(maxdisp=192, struct_fea_c=4, fuse_mode=\"separate\",\n",
    "           affinity_settings=affinity_settings, udc=True, refine=\"csr\")\n",
    "\n",
    "model = nn.DataParallel(model)\n",
    "ckpt = torch.load(\"models/lacGwcNet/checkpoint_9.tar\", map_location=\"cpu\")\n",
    "model.load_state_dict(ckpt[\"net\"])\n",
    "model.cpu()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33499240",
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit -n 2 -r 5 model(left_img, right_img, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2d9839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "#Note: disparity range is tuned according to specific parameters obtained through trial and error. \n",
    "win_size = 5\n",
    "min_disp = -1\n",
    "max_disp = 191 #min_disp * 9\n",
    "num_disp = max_disp - min_disp # Needs to be divisible by 16\n",
    "#Create Block matching object. \n",
    "sgbm = cv2.StereoSGBM_create(minDisparity= min_disp,\n",
    "numDisparities = num_disp,\n",
    "blockSize = 5,\n",
    "uniquenessRatio = 5,\n",
    "speckleWindowSize = 5,\n",
    "speckleRange = 5,\n",
    "disp12MaxDiff = 1,\n",
    "P1 = 8*3*win_size**2,#8*3*win_size**2,\n",
    "P2 =32*3*win_size**2) #32*3*win_size**2)\n",
    "\n",
    "list_filename = \"./filenames/DS_test.txt\"\n",
    "lines = read_all_lines(list_filename)\n",
    "splits = [line.split() for line in lines]\n",
    "left_filenames = [x[0] for x in splits]\n",
    "right_filenames = [x[1] for x in splits]\n",
    "disp_filenames = [x[2] for x in splits]\n",
    "\n",
    "def load_image(filename):\n",
    "    return Image.open(filename).convert('RGB')\n",
    "\n",
    "def load_disp(filename):\n",
    "    data = Image.open(filename)\n",
    "    data = np.array(data, dtype=np.float32) / 256.\n",
    "    return data\n",
    "\n",
    "data_index = 9\n",
    "datapath = \"/home/chris/pl_ws/src/stereo_pl_nav/datasets/DS\"\n",
    "left_img = load_image(os.path.join(datapath, left_filenames[data_index]))\n",
    "right_img = load_image(os.path.join(datapath, right_filenames[data_index]))\n",
    "disparity = load_disp(os.path.join(datapath, disp_filenames[data_index]))\n",
    "left_img = np.asarray(left_img)\n",
    "right_img = np.asarray(right_img)\n",
    "left_gray = cv2.cvtColor(left_img, cv2.COLOR_RGB2GRAY)\n",
    "right_gray = cv2.cvtColor(right_img, cv2.COLOR_RGB2GRAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab295f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "211 ms ± 380 µs per loop (mean ± std. dev. of 5 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit -n 10 -r 5 sgbm.compute(left_gray,right_gray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1c91e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Voxel2D import Voxel2D\n",
    "model = Voxel2D(192, \"voxel\")\n",
    "model = nn.DataParallel(model)\n",
    "ckpt_path = \"/home/chris/pl_ws/src/stereo_pl_nav/scripts/voxelstereonet/logs/lr_0.001_batch_size_16_cost_vol_type_voxel_optimizer_adam_/best.ckpt\"\n",
    "state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict['model'])\n",
    "model.cpu()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5853dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera intrinsics and extrinsics\n",
    "c_u = 4.556890e+2\n",
    "c_v = 1.976634e+2\n",
    "f_u = 1.003556e+3\n",
    "f_v = 1.003556e+3\n",
    "b_x = 0.0\n",
    "b_y = 0.0\n",
    "baseline = 0.54\n",
    "# calculate voxel cost volume disparity set\n",
    "vox_cost_vol_disp_set = set()\n",
    "max_disp = 192\n",
    "# depth starting from voxel_size since 0 will cause issue\n",
    "for z in np.arange(0.5, 32, 2.0):\n",
    "    # get respective disparity\n",
    "    d = f_u * baseline / z\n",
    "\n",
    "    if d > max_disp:\n",
    "        continue\n",
    "\n",
    "    # real disparity -> disparity in feature map\n",
    "    vox_cost_vol_disp_set.add(round(d/4))\n",
    "\n",
    "vox_cost_vol_disps = list(vox_cost_vol_disp_set)\n",
    "vox_cost_vol_disps = sorted(vox_cost_vol_disps)\n",
    "\n",
    "tmp = []\n",
    "for i in vox_cost_vol_disps:\n",
    "    tmp.append(torch.unsqueeze(torch.Tensor([i]), 0))\n",
    "vox_cost_vol_disps = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad6625b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vox_cost_vol_disps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb2601c",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_img = torch.rand((1,3, 400, 880))\n",
    "right_img = torch.rand((1,3, 400, 880))\n",
    "%timeit -n 2 -r 5 model(left_img, right_img, vox_cost_vol_disps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d285a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.Voxel2D_lite import Voxel2D\n",
    "model = Voxel2D(192, \"voxel\")\n",
    "model = nn.DataParallel(model)\n",
    "ckpt_path = \"/home/chris/pl_ws/src/stereo_pl_nav/scripts/voxelstereonet/logs/lr_0.001_batch_size_32_cost_vol_type_voxel_optimizer_adam_lite/best.ckpt\"\n",
    "state_dict = torch.load(ckpt_path, map_location=\"cpu\")\n",
    "model.load_state_dict(state_dict['model'])\n",
    "model.cpu()\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8243a999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera intrinsics and extrinsics\n",
    "c_u = 4.556890e+2\n",
    "c_v = 1.976634e+2\n",
    "f_u = 1.003556e+3\n",
    "f_v = 1.003556e+3\n",
    "b_x = 0.0\n",
    "b_y = 0.0\n",
    "baseline = 0.54\n",
    "# calculate voxel cost volume disparity set\n",
    "vox_cost_vol_disp_set = set()\n",
    "max_disp = 192\n",
    "# depth starting from voxel_size since 0 will cause issue\n",
    "for z in np.arange(0.5, 32, 0.5):\n",
    "    # get respective disparity\n",
    "    d = f_u * baseline / z\n",
    "\n",
    "    if d > max_disp:\n",
    "        continue\n",
    "\n",
    "    # real disparity -> disparity in feature map\n",
    "    vox_cost_vol_disp_set.add(round(d/8))\n",
    "\n",
    "vox_cost_vol_disps = list(vox_cost_vol_disp_set)\n",
    "vox_cost_vol_disps = sorted(vox_cost_vol_disps)\n",
    "\n",
    "tmp = []\n",
    "for i in vox_cost_vol_disps:\n",
    "    tmp.append(torch.unsqueeze(torch.Tensor([i]), 0))\n",
    "vox_cost_vol_disps = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ed73fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vox_cost_vol_disps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926b630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "left_img = torch.rand((1,3, 400, 880))\n",
    "right_img = torch.rand((1,3, 400, 880))\n",
    "%timeit -n 2 -r 5 model(left_img, right_img, vox_cost_vol_disps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1999fe7c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
